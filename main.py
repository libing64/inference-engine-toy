# -*- coding: utf-8 -*-
"""
ä¸»ç¨‹åºå…¥å£ - æ¨¡å‹æŸ¥çœ‹å™¨å’Œæ¨ç†å¼•æ“çš„ç»Ÿä¸€ç•Œé¢
Main Entry Point - Unified interface for model viewer and inference engine
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.prompt import Prompt, Confirm
from typing import Optional

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from model_viewer import ModelViewer
from inference_engine import InferenceEngine

# å¯¼å…¥ç¤ºä¾‹æ¨¡å‹ç±»
try:
    from examples import SimpleCNN, SimpleMLP, SimpleResNet
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹ç±»
    class SimpleCNN(nn.Module):
        def __init__(self, num_classes=10):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 32, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
            )
            self.classifier = nn.Sequential(
                nn.Dropout(),
                nn.Linear(64 * 8 * 8, 128),
                nn.ReLU(),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = torch.flatten(x, 1)
            return self.classifier(x)


class ModelAnalyzer:
    """æ¨¡å‹åˆ†æå™¨ - æ•´åˆæŸ¥çœ‹å™¨å’Œæ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.console = Console()
        self.viewer = ModelViewer()
        self.engine = InferenceEngine(verbose=True)
        self.current_model = None
        
    def welcome_message(self):
        """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
        welcome_text = Text()
        welcome_text.append("ğŸ” ", style="bold blue")
        welcome_text.append("æ¨¡å‹æŸ¥çœ‹å™¨å’Œæ¨ç†å¼•æ“", style="bold green")
        welcome_text.append(" ğŸš€\n\n", style="bold blue")
        welcome_text.append("åŠŸèƒ½è¯´æ˜:\n", style="bold")
        welcome_text.append("â€¢ åŠ è½½å¹¶æŸ¥çœ‹PyTorchæ¨¡å‹ç»“æ„\n", style="cyan")
        welcome_text.append("â€¢ æ‰§è¡Œæ¨¡å‹æ¨ç†å¹¶æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹\n", style="cyan")
        welcome_text.append("â€¢ æ€§èƒ½åŸºå‡†æµ‹è¯•\n", style="cyan")
        welcome_text.append("â€¢ å¯¼å‡ºæ¨¡å‹ä¿¡æ¯\n", style="cyan")
        
        panel = Panel(welcome_text, title="æ¬¢è¿ä½¿ç”¨", border_style="green")
        self.console.print(panel)
    
    def interactive_mode(self):
        """äº¤äº’å¼æ¨¡å¼"""
        self.welcome_message()
        
        while True:
            self.console.print("\n" + "="*50)
            self.console.print("[bold yellow]è¯·é€‰æ‹©æ“ä½œ:[/bold yellow]")
            self.console.print("1. ğŸ” åŠ è½½æ¨¡å‹")
            self.console.print("2. ğŸ“Š æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯")
            self.console.print("3. ğŸ—ï¸  æŸ¥çœ‹æ¨¡å‹æ¶æ„")
            self.console.print("4. ğŸ“‹ æŸ¥çœ‹å±‚è¯¦æƒ…")
            self.console.print("5. ğŸ“ è¿½è¸ªæ¨¡å‹å½¢çŠ¶")
            self.console.print("6. âš¡ æ¨¡å‹æ¨ç†")
            self.console.print("7. ğŸƒ æ€§èƒ½æµ‹è¯•")
            self.console.print("8. ğŸ’¾ å¯¼å‡ºä¿¡æ¯")
            self.console.print("0. ğŸ‘‹ é€€å‡ºç¨‹åº")
            
            choice = Prompt.ask("è¯·è¾“å…¥é€‰æ‹©", choices=["0", "1", "2", "3", "4", "5", "6", "7", "8"])
            
            try:
                if choice == "0":
                    self.console.print("[green]ğŸ‘‹ å†è§![/green]")
                    break
                elif choice == "1":
                    self._handle_load_model()
                elif choice == "2":
                    self._handle_model_info()
                elif choice == "3":
                    self._handle_model_architecture()
                elif choice == "4":
                    self._handle_layer_details()
                elif choice == "5":
                    self._handle_trace_shapes()
                elif choice == "6":
                    self._handle_inference()
                elif choice == "7":
                    self._handle_benchmark()
                elif choice == "8":
                    self._handle_export()
                    
            except KeyboardInterrupt:
                self.console.print("\n[yellow]æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
            except Exception as e:
                self.console.print(f"[red]æ“ä½œå‡ºé”™: {str(e)}[/red]")
    
    def _handle_load_model(self):
        """å¤„ç†æ¨¡å‹åŠ è½½"""
        model_path = Prompt.ask("è¯·è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.pth)")
        
        if not os.path.exists(model_path):
            self.console.print("[red]æ–‡ä»¶ä¸å­˜åœ¨![/red]")
            return
        
        # åŠ è½½åˆ°æŸ¥çœ‹å™¨
        if self.viewer.load_model(model_path):
            self.console.print("[green]âœ“ æ¨¡å‹å·²åŠ è½½åˆ°æŸ¥çœ‹å™¨[/green]")
            
            # å°è¯•åŠ è½½åˆ°æ¨ç†å¼•æ“
            if hasattr(self.viewer, 'model') and self.viewer.model is not None:
                self.engine.load_model(self.viewer.model)
                self.current_model = self.viewer.model
                self.console.print("[green]âœ“ æ¨¡å‹å·²åŠ è½½åˆ°æ¨ç†å¼•æ“[/green]")
            else:
                self.console.print("[yellow]âš ï¸ åªæœ‰æƒé‡æ–‡ä»¶ï¼Œæ¨ç†åŠŸèƒ½éœ€è¦å®Œæ•´æ¨¡å‹[/yellow]")
        else:
            self.console.print("[red]æ¨¡å‹åŠ è½½å¤±è´¥![/red]")
    
    def _handle_model_info(self):
        """å¤„ç†æ¨¡å‹ä¿¡æ¯æ˜¾ç¤º"""
        if not self._check_model_loaded():
            return
        
        self.viewer.display_model_summary()
        
        # å¦‚æœåªæœ‰state_dictï¼Œä¹Ÿæ˜¾ç¤ºç›¸å…³ä¿¡æ¯
        if 'state_dict' in self.viewer.model_info:
            self.viewer.display_state_dict_info()
    
    def _handle_model_architecture(self):
        """å¤„ç†æ¨¡å‹æ¶æ„æ˜¾ç¤º"""
        if not self._check_model_loaded():
            return
        
        if hasattr(self.viewer, 'model') and self.viewer.model is not None:
            self.viewer.display_model_architecture()
        else:
            self.console.print("[yellow]åªæœ‰æƒé‡ä¿¡æ¯ï¼Œæ— æ³•æ˜¾ç¤ºå®Œæ•´æ¶æ„[/yellow]")
            self.viewer.display_state_dict_info()
    
    def _ensure_shape_info(self):
        """ç¡®ä¿æ¨¡å‹åŒ…å«å½¢çŠ¶ä¿¡æ¯"""
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å½¢çŠ¶ä¿¡æ¯ï¼ˆé€šè¿‡æ£€æŸ¥ç¬¬ä¸€å±‚çš„input_shapeæ˜¯å¦ä¸ºUnknownï¼‰
        if self.viewer.model_info.get('layers') and \
           self.viewer.model_info['layers'][0].get('input_shape') == 'Unknown':
            
            if Confirm.ask("æ¨¡å‹ç¼ºå°‘è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¿¡æ¯ï¼Œæ˜¯å¦ç°åœ¨è¿›è¡Œè¿½è¸ª?", default=True):
                try:
                    input_shape = Prompt.ask("è¯·è¾“å…¥è¾“å…¥æ•°æ®å½¢çŠ¶ (ä¾‹å¦‚: 1,3,224,224)")
                    shape_list = [int(x.strip()) for x in input_shape.split(',')]
                    self.viewer.trace_model_shapes(tuple(shape_list))
                except ValueError:
                    self.console.print("[red]è¾“å…¥æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡å½¢çŠ¶è¿½è¸ª[/red]")

    def _handle_layer_details(self):
        """å¤„ç†å±‚è¯¦æƒ…æ˜¾ç¤º"""
        if not self._check_model_loaded():
            return
            
        self._ensure_shape_info()
        self.viewer.display_layer_details()

    def _handle_trace_shapes(self):
        """å¤„ç†æ¨¡å‹å½¢çŠ¶è¿½è¸ª"""
        if not self._check_model_loaded():
            return
            
        try:
            input_shape = Prompt.ask("è¯·è¾“å…¥è¾“å…¥æ•°æ®å½¢çŠ¶ (ä¾‹å¦‚: 1,3,224,224)")
            shape_list = [int(x.strip()) for x in input_shape.split(',')]
            self.viewer.trace_model_shapes(tuple(shape_list))
        except ValueError:
            self.console.print("[red]è¾“å…¥æ ¼å¼æ— æ•ˆ[/red]")

    def _handle_inference(self):
        """å¤„ç†æ¨¡å‹æ¨ç†"""
        if not self._check_inference_ready():
            return
        
        # è·å–è¾“å…¥å‚æ•°
        try:
            input_shape = Prompt.ask("è¯·è¾“å…¥æ•°æ®å½¢çŠ¶ (ä¾‹å¦‚: 1,3,224,224)")
            shape_list = [int(x.strip()) for x in input_shape.split(',')]
            
            # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
            input_data = torch.randn(*shape_list)
            self.console.print(f"[green]å·²åˆ›å»ºéšæœºè¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶: {list(input_data.shape)}[/green]")
            
            # æ‰§è¡Œæ¨ç†
            result = self.engine.infer(input_data, detailed=True)
            self.console.print(f"\n[bold green]æ¨ç†ç»“æœå½¢çŠ¶: {list(result.shape)}[/bold green]")
            
            # è¯¢é—®æ˜¯å¦æ˜¾ç¤ºç»“æœæ•°æ®
            if Confirm.ask("æ˜¯å¦æ˜¾ç¤ºè¾“å‡ºæ•°æ®?"):
                self.console.print(f"è¾“å‡ºæ•°æ®:\n{result}")
            
        except ValueError as e:
            self.console.print(f"[red]è¾“å…¥æ ¼å¼é”™è¯¯: {str(e)}[/red]")
        except Exception as e:
            self.console.print(f"[red]æ¨ç†å¤±è´¥: {str(e)}[/red]")
    
    def _handle_benchmark(self):
        """å¤„ç†æ€§èƒ½æµ‹è¯•"""
        if not self._check_inference_ready():
            return
        
        try:
            input_shape = Prompt.ask("è¯·è¾“å…¥æµ‹è¯•æ•°æ®å½¢çŠ¶ (ä¾‹å¦‚: 1,3,224,224)")
            shape_list = [int(x.strip()) for x in input_shape.split(',')]
            
            num_runs = int(Prompt.ask("è¯·è¾“å…¥æµ‹è¯•æ¬¡æ•°", default="10"))
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            input_data = torch.randn(*shape_list)
            
            # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
            stats = self.engine.benchmark_model(input_data, num_runs)
            
        except ValueError as e:
            self.console.print(f"[red]è¾“å…¥é”™è¯¯: {str(e)}[/red]")
        except Exception as e:
            self.console.print(f"[red]åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}[/red]")
    
    def _handle_export(self):
        """å¤„ç†ä¿¡æ¯å¯¼å‡º"""
        if not self._check_model_loaded():
            return
        
        self._ensure_shape_info()
        output_path = Prompt.ask("è¯·è¾“å…¥å¯¼å‡ºæ–‡ä»¶è·¯å¾„", default="model_info.json")
        self.viewer.export_model_info(output_path)
    
    def _check_model_loaded(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²åŠ è½½æ¨¡å‹"""
        if not self.viewer.model_info:
            self.console.print("[red]è¯·å…ˆåŠ è½½æ¨¡å‹![/red]")
            return False
        return True
    
    def _check_inference_ready(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œæ¨ç†"""
        if not hasattr(self.engine, 'model'):
            self.console.print("[red]è¯·å…ˆåŠ è½½å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶![/red]")
            return False
        return True
    
    def load_model_from_path(self, model_path: str) -> bool:
        """ä»å‘½ä»¤è¡Œç›´æ¥åŠ è½½æ¨¡å‹"""
        return self.viewer.load_model(model_path)
    
    def quick_analysis(self, model_path: str):
        """å¿«é€Ÿåˆ†ææ¨¡å¼"""
        if self.load_model_from_path(model_path):
            self.console.print("\n[bold yellow]ğŸ“Š æ¨¡å‹ä¿¡æ¯æ¦‚è§ˆ:[/bold yellow]")
            self.viewer.display_model_summary()
            
            self.console.print("\n[bold yellow]ğŸ—ï¸ æ¨¡å‹æ¶æ„:[/bold yellow]")
            if hasattr(self.viewer, 'model') and self.viewer.model is not None:
                self.viewer.display_model_architecture()
            else:
                self.viewer.display_state_dict_info()


def create_sample_model():
    """åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ¨¡å‹ç”¨äºæµ‹è¯•"""
    class SimpleNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Dropout(),
                nn.Linear(128 * 56 * 56, 512),
                nn.ReLU(inplace=True),
                nn.Dropout(),
                nn.Linear(512, 10),
            )
        
        def forward(self, x):
            x = self.features(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x
    
    return SimpleNet()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ¨¡å‹æŸ¥çœ‹å™¨å’Œæ¨ç†å¼•æ“")
    parser.add_argument('--model', '-m', type=str, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--interactive', '-i', action='store_true', help='å¯åŠ¨äº¤äº’æ¨¡å¼')
    parser.add_argument('--quick', '-q', action='store_true', help='å¿«é€Ÿåˆ†ææ¨¡å¼')
    parser.add_argument('--create-sample', action='store_true', help='åˆ›å»ºç¤ºä¾‹æ¨¡å‹')
    
    args = parser.parse_args()
    
    analyzer = ModelAnalyzer()
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
    if args.create_sample:
        sample_model = create_sample_model()
        sample_path = "sample_model.pth"
        torch.save(sample_model, sample_path)
        analyzer.console.print(f"[green]âœ“ ç¤ºä¾‹æ¨¡å‹å·²ä¿å­˜åˆ°: {sample_path}[/green]")
        return
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹æ–‡ä»¶
    if args.model:
        if args.quick:
            # å¿«é€Ÿåˆ†ææ¨¡å¼
            analyzer.quick_analysis(args.model)
        else:
            # åŠ è½½æ¨¡å‹åè¿›å…¥äº¤äº’æ¨¡å¼
            if analyzer.load_model_from_path(args.model):
                if args.interactive:
                    analyzer.interactive_mode()
            else:
                analyzer.console.print("[red]æ¨¡å‹åŠ è½½å¤±è´¥![/red]")
                sys.exit(1)
    else:
        # é»˜è®¤è¿›å…¥äº¤äº’æ¨¡å¼
        analyzer.interactive_mode()


if __name__ == "__main__":
    main()
